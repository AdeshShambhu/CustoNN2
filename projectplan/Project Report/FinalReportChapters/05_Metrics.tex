\chapter{Metrics} \label{chp:Metrics}
Our objective was to leverage the infrastructure available at the Paderborn Centre for Parallel Computing (PC2) to build an inference accelerator system. We evaluated the effectiveness of our inference accelerator system by considering two classes of metrics - performance metrics and quality metrics.  

Performance metrics just account for the arithmetic operations performed on the FPGA. They do not tell whether the operations being performed are functionally correct or not. To account for the quality of the operations being performed, we considered accuracy, which is defined a little further down in this chapter. As we have used existing, pre-trained network topologies, we aimed to attain (or reach as close as possible to)  the accuracy mentioned in the literature.

\section{Performance Metrics}

\subsection{Operations per cycle}
Operations per cycle is a unit of measure for the numerical computing performance of a computer. Typically expressed as OPs/cycle, this metric tells us how many operations are performed in one clock cycle of the targeted hardware.  

To calculate OPs/cycle we looked at the source code and the final profiler report from the executed code. From the source code, we calculated the number of operations sans the overheads (loop index calculations, loop increment operations, etc). From the profiler report, we got a concrete number for the clock frequency of the FPGA and the execution time. Combining these two allowed us to calculate operations per cycle.

\subsection{Operations per second}
Operations per second is closely related with the above metric “Operations per cycle”. It is usually expressed as OPs/second. OPs/second depends on the clock frequency of the platform. 
We calculated “operations per second” by simply dividing the number of operations by the execution time.  
As per literature mentioned in Chapter 7 named Related Work, popular tool-flows like DNNWEAVER on Arria 10 GX115 have achieved 184.33GOPs/second, running at a clock frequency of 200MHz.

\subsection{Operations per byte}
Operations per byte or Arithmetic Intensity is the number of operations performed per byte of data transferred between the FPGA and off-chip memory.  
In our case, we calculated the number of operations performed and the total amount of data transferred by looking at the source code. We also looked at the profiler report at the end of execution to get the correct estimation of data transferred between global memory and local memory.  
This metric helped us to identify if we are memory bandwidth limited or compute resource-bounded.  

The metrics OPs/second and OPs/byte made up the metrics required to do Roofline Analysis of our inference system.  


\subsection{Latency}
Latency is generally defined as the time delay between initial input to a system and the output from the system. Latency captures the time it takes to load data, pre-process it, send said data over a network to the Inference Engine, the time needed for inference, and time needed to send the classified data back to the user. 

In our case, the focus was on improving the time needed for inference and hence we redefined latency to refer to only the amount of time to infer an image on FPGAs ( i.e, without considering the overheads).

Latency can be measured for just a single image or over a batch of images. We can expect to have different latencies based on how we measure it (single image vs batch of images). As an example of what has been achieved in related works, we have ALAMO tool-flow running on Arria 10 GT115 running at 239.62MHz having a latency of 4.05ms.
We aimed to have latency in this range.

\subsection{Throughput}
Throughput is a measure of the amount of information processed per unit time. In our case, we defined it as the number of images classified per second. There are well-known techniques to improve throughput like batch processing where a number of images are batched together and sent for inference job. 
But as the batch size goes up, latency also tends to go up. So there is a trade-off involved here between throughput and latency.  \par
We had deployed a pre-trained ResNet-50 DNN model in the Microsoft Azure cloud during our research phase and the images were classified in less than 4 ms and a maximum throughput of 39.5 Teraflops on a single Stratix 10 FPGA. Hence, we decided to benchmark our project group's implementation against Microsoft Project Brainwave.



\section{Quality Metrics}
\subsection{Accuracy}
Accuracy is defined as the fraction of the number of correct inferences made to the total number of inferences made. In our context, accuracy depended on the CNN topology used, nature of weights (floating point vs fixed points, etc), training data, etc. 
In literature, accuracy is expressed in terms of percentages and the changes from baseline model's accuracy is measured in terms of percentage points (pp). 
In our case, baseline models were the existing, trained topologies mentioned in Chapter \ref{chp:DataTopoArch}. Our goal was to make use of DNN approximation techniques and still achieve accuracy within 3pp of the baseline model.
\newline
Since we made use of pre-trained models in this project, there was no change in accuracy of classification given by our Inference Engine.
To check the accuracy of the label and score given by the Inference Engine, the pre-trained models were executed in TensorFlow and output for the same were retrieved. If the output score of the Inference Engine was same as that given by TensorFlow then the classification was deemed accurate else changes had to be made to output the correct score.

\subsection{Rate Correct Score}
Taking inspiration from the field of Memory and Cognitive research, we combined accuracy and latency by considering Rate Correct Score. It is defined as 

\begin{equation} \label{eqt:example_equation}
RCS = \frac{c}{\sum R_t}
\end{equation}
\equations{Rate Correct Score}

%$RCS = \frac{c}{\sum R_t}$    

where   

$c$ = accuracy    

      $\sum R_t$ = batch execution time  
      
We aimed at maximizing RCS throughout the entire runtime of the project (either by increasing the accuracy or by decreasing the latency or both).
