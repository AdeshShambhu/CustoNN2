\chapter{Goals}
Convolution Neural Network (CNN) is a type of Deep Neural Network (DNN) used in the field of image processing and image analyzing. CNNs are artificial neural network that have so far been popularly used for analyzing images. The role of the CNNs are to reduce the images into a form such that it is easier to process, without losing features which are critical for getting a good prediction. The objective of CNN is to extract features from the input images. 
The main focus of our project is to implement the state-of-the-art CNNs on the FPGAs which acts as an accelerator for compute-heavy neural networks. We have the following sub-goals:
\section{Scaling over multiple FPGAs}
An FPGA is made up of finite and limited resources and implementing these large CNN models can often result in running out of resources and cannot fit into a single FPGA. Hence in this project, we have used the FPGA infrastructure in the Noctua cluster. The Noctua cluster is a high-performance computing system equipped with 32 Intel Stratix 10 FPGAs with point to point connections. We have also scaled our CNN models on the Noctua cluster by partitioning the large model into smaller models based on the number of layers that will fit onto a single FPGA and these smaller models were executed on multiple FPGAs to provide low latency computation. We have used the OpenCL external I/O channels to transfer intermediate results from one FPGA to another. By scaling our application on multiple FPGAs infrastructure we target to perform as good as the Microsoft's Project Brainwave architecture in terms of latency and throughput.
\par
Microsoft Project Brainwave is a deep learning platform for real-time Artificial Intelligence (AI) applications like computer vision. Brainwave is equipped with Intel Stratix 10 FPGAs in the Microsoft Azure Cloud and Neural Processing Units (NPU) architecture for executing a DNN model with low latency and high throughput. Brainwave also supports multiple FPGAs infrastructure in the Azure cloud. When the memory resource of the FPGA is exhausted by the DNN model layers, the remaining layers will be mapped to the next FPGA enabling scaling.

\section{Performance Optimization}
Heterogeneous computing systems provide performance gain by using specialized capabilities of different types of processors (CPU+FPGA in our project). OpenCL is a framework for heterogeneous computing which provides software-centric development flow for FPGAs and obtains performance and power advantages with hardware acceleration. We have used the OpenCL tool flow for developing CNN models in our project inorder to develop individual kernels for each layer of the CNN model. Also, we considered the concept of channels offered by OpenCL to transfer intermediate results from one layer to the next layer without writing the results in the global memory. The kernels were further optimized by using several OpenCL optimization techniques learnt during the tutorial phase eg: loop unrolling, reducing the Initiation Interval (II) of loops to 1, removing memory dependencies, etc. Performance modeling was applied to each OpenCL kernels to identify the bottlenecks in the design and resolve the issues to obtain  high throughput and high utilization.

\section{Quantization }
CNN implementation is often complex due to the large number of parameters and computations, which makes it difficult to be implemented on a resource constrained FPGA. The floating point operations are computationally expensive, hence quantization can be applied to the deep networks to convert the floating point weights into fixed point weights. Fixed point operations typically consumes fewer hardware resources than floating point operations. Quantization will help in reducing the power consumption, memory footprint and resources utilization. In our project, we rely on Intel OpenVINO to optimize and quantize the CNN model so that the accuracy is not drastically dropped in the quantized model. We will also distinguish the models with quantized weights and floating point weights.
