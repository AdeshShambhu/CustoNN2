\chapter{Related Work}
A lot of research has been going on in the field of enhancing the computation speed of CNNs on FPGA for quite some time. In this section, we will take a look at the various research work that has been done so far. We know that CNNs consist of different layers such as convolution, pooling, and fully connected layers, out of which convolution forms a major part of it. \cite{DBLP:conf/iccv/LiuLSHYZ17} implemented convolution layer using sequential nested \textit{for} loops. In our project, we are also using sequential nested for loops for the implementation of convolution layer.

We also know that FPGAs and ASICs increase the speed of inference, while GPUs still excel at floating point operations. CNNs generally work on floating-point representation but working with them comes at a cost in the form of computation time. The need of the hour is to have high throughput and high efficiency. Hence it is better to use data in a fixed-point representation. To cite a few, most of the research work of \cite{DBLP:journals/corr/abs-1712-05877}, \cite{DBLP:conf/fpga/QiuWYGLZYTXSWY16}, \cite{DBLP:conf/icassp/ShinHS16} has been done in the field of quantization in order to improve computation speed through low bit representation. \cite{DBLP:journals/corr/CourbariauxBD14} implemented block floating point and their CNN topology gave a 1.0 pp accuracy loss. \cite{DBLP:conf/fpga/QiuWYGLZYTXSWY16} used an 8-bit representation of weights on ImageNet and achieved a 0.4 pp accuracy loss. Similarly, other techniques like binarization, ternarization, etc., are also implemented and these techniques showed better performance than the standard CNN which used FP32 bit representation. In our project, we are trying to quantize weights in 16-bit representation because by doing quantization of weights in 16-bit we will have a mere loss of 0.1pp in accuracy and also it will help us to achieve our goal of high throughput.

In 2018 a survey was carried out which discussed various tool-flows for mapping of CNN on FPGAs. These tool-flows were distinguished into two categories based on the architecture each tool-flow adheres to. First architecture is \textit{Streaming Architecture} where each layer is assigned a hardware block and these layers are chained to form a pipeline\cite{DBLP:journals/tnn/DundarJMC17}. Second architecture is \textit{Single Computation Engine} architecture which is used to form an overlay. Due to this, it provides flexibility, portability, fast configuration time. In our project, we are trying to create overlays, where we will create a separate .aocx file for each layer and then these can be called in the OpenVINO plugin to deploy it on FPGAs.

