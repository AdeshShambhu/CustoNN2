\chapter{Comparison with state-of-the-art benchmarks and challenges faced}

\begin{table}[!htb]
\centering
\captionsetup{
justification = centering
}
\caption{Comparison with state-of-the-art benchmarks}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Topology} & \textbf{\begin{tabular}[c]{@{}c@{}}FPGA \\ execution time\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}OpenVINO CPU  \\ execution time\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Microsoft \\ Brainwave\end{tabular}} \\ \hline
GoogLeNet & 1.18 sec  & 8.79 ms & -\\ \hline
ResNet–50 & 10.77 sec & 19.42 ms & 4 ms \\ \hline
\end{tabular}
\label{tab:Comp_benchmarks}

\end{table}
Table \ref{tab:Comp_benchmarks} shows the comparison of execution times of different topologies on different platforms and on a deep learning platform, Microsoft Brainwave. OpenVINO CPU plugin was executed on "cc-frontend" with 16 CPUs, 4 cores in each socket running at an average clock frequency of 1.2 GHz.  OpenVINO FPGA plugin was executed on multiple Intel Stratix 10 FPGAs in the Noctua infrastructure and Microsoft Brainwave executes the CNN topologies on one Stratix 10 FPGA.

For GoogLeNet topology, execution time on OpenVINO CPU plugin was 8.79ms and on OpenVINO FPGA plugin was 1.18s. Microsoft Brainwave does not support this topology. Whereas for ResNet topology, execution time on OpenVINO CPU plugin was 19.42ms, on OpenVINO FPGA plugin was 10.77s and Microsoft Brainwave took execution time of 4ms on 1 FPGA.

 We observed that the difference in execution time of the topologies is very vast between FPGA plugin and CPU plugin. Hence, we examined deep to find out the probable causes for this difference. 

%End of Chapter
\section{Causes of sub-optimal results} 
\label{sec:challenges}

\begin{enumerate}

\item\textbf{Limited parallel operations: } The OpenVINO CPU plugin is highly optimized to run on CPUs. The CPU on which the OpenVINO CPU plugin was run is a multicore system with a maximum operating clock frequency of 3.2GHz. We can infer from this that the OpenVINO CPU plugin was able to exploit the parallelism offered by the multicore systems to execute the design in a highly parallel manner. Approximately, the OpenVINO CPU plugin was able to execute about 55 Operations per cycle.
On the other hand, the best we could do was 12 Operations per cycle for short durations in GoogLeNet hybrid design.

\item\textbf{Serial Execution of the Designs: }
The GoogleNet and ResNet CNN models have a sequential architecture. Since the whole network is too big to fit in one FPGA, we divide the structure into several small sub-structure and then execute them on several FPGAs. Due to sequential architecture, the next sub-structure is dependent on the previous sub-structure, hence the next FPGA has to wait till the previous FPGA completes the execution. 
This problem can be overcome by modifying the design to support batches of images.


\item\textbf{Accumulated Data based logic: }
The kernel design that we have used in GoogleNet hybrid design makes use of internal and external IO channels. These kernels work on accumulated data, i.e., the kernel starts processing after only it has received the complete set of data from the channels. Hence the kernels here are idle till they receive the whole data. One way to solve this issue is to use a streaming design. Streaming design has data pipelined in its architecture thereby eradicating the idle time of the kernels.

\item\textbf{Limited design space exploration: }
Several factors come in to play during the synthesis of kernels. In our design, we observed a positive correlation between the amount of DSP block usage and synthesis time. Due to some of the designs taking upto 24 hours to get synthesized, it was very challenging to iterate over different optimization points. This meant we could not bring to fruition some of the optimization ideas we had in mind.

The design space for applications running on FPGAs is very big. We had no control over the final fmax as this is determined by the compiler. But we noticed a correlation between DSP usage and fmax. If we were too aggressive with increasing DSP usage, then fmax would go down considerably. If not enough DSP blocks were requested for (by not unrolling the compute loops), then we would get a good fmax but compute parallelism would go down. So we had to explore this large design space to find the right balance between DSP blocks and fmax. And we could not really cover much of this design space.


\item\textbf{Lack of Experience: }
The ability to find the right balance between different parameters, to determine the right number of FPGAs to scale over, to debug  stalled kernels etc is something which comes over time. The lack of experience meant we were not able to figure out the best design paradigms to implement different CNNs. On the other hand, the lessons learnt during different design points  helped us accelerate at the later stages of the project. With time and experience, better solutions can be implemented.

\end{enumerate}


\section{Quantization Issues }
One of the initial goals listed in the project plan was to perform Quantization. Quantization involves low precision math and hence can positively impact various performance metrics of our project. This was the motivation behind choosing this subgoal. Among various approaches available to achieve this, after discussion and research, we decided to implement Post Training Quantization which is supported by both TVM and OpenVINO. The Calibration Tool present in the latest release of OpenVINO performs post-training quantization of the CNN model to int8 before the model’s IR is passed on to the inference engine. This task is less time consuming and less complex as we don’t have to retrain the model using quantized weights. Also, we were able to generate kernels using TVM’s quantize function. 

Although most of the aspects seemed to fall into place, we came across two potential risks that hindered us in achieving this goal. 
		
\begin{itemize}
\item The version of OpenVINO used in the project (2018 R5) only supports FP16 or FP32 operations. The Post Training Quantization discussed previously is available in the latest version (2019 R1). This implied that we should migrate our project 2018 R5 version to the new one. During that time of the project, many branches of the repository were simultaneously under development and it made migration difficult. 
\item To support migration to a new version of OpenVINO, we also may be required to make modifications to the existing FPGA plugin and again, this would involve understanding the changes, impact analysis and development. Due to the limited amount of time available, even this affected our decision to stop pursuing this goal.
\end{itemize}
