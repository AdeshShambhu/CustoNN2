# GoogLeNet Baseline Architecture
This directory contains the OpenCL kernels for GoogLeNet. The Deep Neural Network(DNN) is split linearly in various points over the network and the division of the network was based
on inception modules. Each OpenCL kernel files contain one inception module.  
In this design we are using the kernels generated by TVM for Googlenet, hence we are using FPGA global memory throughout the network.
We have generated bitstreams for each of these files and these bitstreams will be flashed onto nine Stratix 10 FPGAs available in Noctua cluster. So, we need 5 Noctua nodes for inference of GoogLeNet. Intermediate
results between kernels inside the OpenCL file will be transferred using Global memory and we plan to use MPI for transfer of intermediate results between devices and nodes.

- Bitstream location : `/upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/designs/googlenet_bitstreams`
- Command to generate bitstream : `make inception_<module_name>`  
    If you wish to change the directory of bitstream, please change the value of `design_dir` in the Makefile.


## Steps to run GoogLenet using OpenVINO FPGA Plugin with MPI on Stratix 10 FPGAs

### Connecting the Noctua Cluster
1. Connect to the Noctua Load Balancer  
    `ssh fe.noctua.pc2.uni-paderborn.de`  
    (This command can be executed from the CC Fe also)

2. Connect to Noctua FPGA front nodes  
    `ssh noctua`

3. Load OpenMPI,CMake and GCC modules  
    - `module load intel/18.0.3`
	- `module load mpi/OpenMPI/1.10.3-GCC-5.4.0-2.26`
	- `module load intelFPGA_pro/19.1.0 nalla_pcie/19.1.0 gcc/6.1.0`
	- `module load devel/CMake/3.6.1-foss-2016b`


    The above procedure has to be followed each time you login to Noctua node.

### Build the OpenVINO noctua plugin containing MPI
 OpenVINO noctua plugin is built to integrate IR from Model Optimizer with OpenCL Kernels generated from TVM and launch these kernels on multiple FPGA's using MPI send and recv functions .
We have developed the plugin for this in `noctua_plugin_develop` branch of `dldt` project. Please switch to this branch if you are in different branch.

 1. Navigate to build directory of OpenVINO inference engine:  
    `cd $<dldt>/inference-engine/build`
2. Run the CMake command. Please skip this step if the plugin is already built  
	`cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_CLDNN=OFF -DENABLE_GNA=OFF ..`
4. Build the plugin  
    `make -j16`  
    If you are building the Inference Engine for the first time , please run Cmake command given this documentation: https://git.uni-paderborn.de/cs-hit/pg-custonn2-2018-3rd-party/dldt/blob/noctua_plugin_develop/inference-engine/build_instructions_pc2.md.

 3. After building the plugin, navigate to bin directory of inference engine:  
    `cd $<dldt>/inference-engine/bin/intel64/DEBUG` If you have built the project in Debug mode   
    `cd $<dldt>/inference-engine/bin/intel64/Release` if the project is built in Release mode.

 4. Request for FPGA nodes using salloc 
    For this implementation we are require 5 Noctua nodes each having 2 FPGAs. Out of 10 FPGAs, we will be using only 9 in this baseline design.
	 `salloc -N 5 --partition=fpga -A hpc-lco-kenter -w fpga-[0005-0009]`
    - `-N` number of nodes
    - `partition` noctua node type
    - `-A` account
    - `-w` selecting particular nodes in the noctua cluster.
 6. Execute the model using mpirun  
    `mpirun -npernode 1 ./test_plugin -m /upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/hakathur/misc/IR/frozen_quant.xml -i /upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/intermediate_representation/pepper.png -label /upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/intermediate_representation/GoogLeNet/labels.txt -nt 10 -bitstream /upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/designs/googlenet_bitstreams/ -model googlenet`  
    - Test Plugin is the user application for executing the plugin
    - `-m` is the path for IR XML
    - `-i` is the path of the Image


