# GoogLeNet Baseline Architecture
This directory contains the OpenCL kernels for GoogLeNet. The Deep Neural Network(DNN) is split linearly in various points over the network and the division of the network was based
on inception modules. Each OpenCL kernel files contain one inception module.  
In this design we are using the kernels generated by TVM for Googlenet, hence we are using FPGA global memory throughout the network.
We have generated bitstreams for each of these files and these bitstreams will be flashed onto nine Stratix 10 FPGAs available in Noctua cluster. So, we need 5 Noctua nodes for inference of GoogLeNet. Intermediate
results between kernels inside the OpenCL file will be transferred using Global memory and we plan to use MPI for transfer of intermediate results between devices and nodes.

- Bitstream location : `/upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2/designs/googlenet_bitstreams`
- Command to generate bitstream : `make inception_<module_name>`  
    If you wish to change the directory of bitstream, please change the value of `design_dir` in the Makefile.


## Steps to run GoogLenet using OpenVINO FPGA Plugin with MPI on Stratix 10 FPGAs

### Connecting the Noctua Cluster
1. Connect to the Noctua Load Balancer  
    `ssh fe.noctua.pc2.uni-paderborn.de`  
    (This command can be executed from the CC Fe also)

2. Connect to Noctua FPGA front nodes  
    `ssh noctua`

3. Load OpenMPI,CMake and GCC modules  
    - `module load intel/18.0.3`
	- `module load mpi/OpenMPI/1.10.3-GCC-5.4.0-2.26`
	- `module load intelFPGA_pro/19.1.0 nalla_pcie/19.1.0 gcc/6.1.0`
	- `module load devel/CMake/3.6.1-foss-2016b`


    The above procedure has to be followed each time you login to Noctua node.

### Build the OpenVINO noctua plugin containing MPI
 OpenVINO noctua plugin is built to integrate IR from Model Optimizer with OpenCL Kernels generated from TVM and launch these kernels on multiple FPGA's using MPI send and recv functions .
We have developed the plugin for this in `noctua_plugin_develop` branch of `dldt` project. Please switch to this branch if you are in different branch.

If you are building the Inference Engine for the first time , please refer to this documentation: https://git.uni-paderborn.de/cs-hit/pg-custonn2-2018-3rd-party/dldt/blob/noctua_plugin_develop/inference-engine/build_instructions_pc2.md.  
 1. Navigate to build directory of OpenVINO inference engine:  
    `cd $<dldt>/inference-engine/build`
2. Run the CMake command. Please skip this step if the plugin is already built  
	`cmake -DCMAKE_BUILD_TYPE=Release -DENABLE_CLDNN=OFF -DENABLE_GNA=OFF ..`
3. Build the plugin  
    `make -j16`  
4. After building the plugin, navigate to bin directory of inference engine:  
    `cd $<dldt>/inference-engine/bin/intel64/DEBUG` If you have built the project in Debug mode   
    `cd $<dldt>/inference-engine/bin/intel64/Release` if the project is built in Release mode.

5. Request for FPGA nodes using salloc 
    For this implementation we are require 5 Noctua nodes each having 2 FPGAs. Out of 10 FPGAs, we will be using only 9 in this baseline design.
	 `salloc -N 5 --partition=fpga -A hpc-lco-kenter -w fpga-[0005-0009]`
    - `-N` number of nodes
    - `-partition` noctua node type
    - `-A` account
    - `-w` selecting particular nodes in the noctua cluster.
6. Execute the model using mpirun  
    To simplify the following command, please initialize a temporary variable with the project group's file directory. <br> `export CUSTONN2=/upb/scratch/departments/pc2/groups/pc2-cc-user/custonn2`   <br><br>
    `mpirun -npernode 1 ./test_plugin -m $CUSTONN2/intermediate_representation/GoogLeNet/frozen_quant.xml -i $CUSTONN2/intermediate_representation/pepper.png -label $CUSTONN2/intermediate_representation/GoogLeNet/labels.txt -nt 10 -bitstream $CUSTONN2/designs/googlenet_bitstreams/ -model googlenet`  
    <br>Test Plugin is the user application for executing the plugin. execute help command to get to know the description of each arguments `./test_plugin -h`  
    - `-m` is the path for IR XML
    - `-i` is the path of the Image
    - `-model` Input CNN Model name. Supported models : googlenet, resnet
    - `-label` Path to the labels.txt file of the model with label indicies and names
    - `-nt`  Number of top results (default 10)
    - `-bitstream` Path to the bitstreams directory  
    MPI Run options 
    - `-npernode` Number of process per node.  




